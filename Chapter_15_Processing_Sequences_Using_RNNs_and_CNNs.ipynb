{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fadhilahmad11/Hands-on-Machine-Learning-with-Scikit-Learn-TensorFlow-Tugas-Machine-LearningW8-W16/blob/main/Chapter_15_Processing_Sequences_Using_RNNs_and_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 15: Processing Sequences Using RNNs and CNNs  \n",
        "\n",
        "---\n",
        "\n",
        "## 1. Pendahuluan  \n",
        "\n",
        "Chapter ini membahas bagaimana memproses data sekuensial (misalnya teks, sinyal waktu) menggunakan Recurrent Neural Networks (RNN) dan CNN.  \n",
        "RNN efektif dalam menangkap dependensi urutan, sedangkan CNN dapat dipakai untuk sequence dengan filter 1D.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. RNN Dasar  \n",
        "\n",
        "RNN memproses input sekuensial dengan memperbarui state hidden di setiap timestep:\n",
        "\n",
        "$\n",
        "h_t = \\phi(W_h h_{t-1} + W_x x_t + b)\n",
        "$\n",
        "\n",
        "di mana:\n",
        "- $( h_t $): hidden state pada waktu t\n",
        "- $( x_t $): input pada waktu t\n",
        "- $( W_h, W_x $): bobot\n",
        "- $( \\phi $): fungsi aktivasi (biasanya tanh atau ReLU)\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Long Short-Term Memory (LSTM)  \n",
        "\n",
        "LSTM mengatasi masalah vanishing gradient dengan menggunakan:\n",
        "- **Forget gate**\n",
        "- **Input gate**\n",
        "- **Output gate**\n",
        "\n",
        "Setiap gate mengontrol aliran informasi melalui sel memori.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Gated Recurrent Unit (GRU)  \n",
        "\n",
        "GRU menyederhanakan LSTM:\n",
        "- Tidak punya sel memori terpisah.\n",
        "- Gabungkan forget dan input gate menjadi update gate.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. CNN untuk Sequence  \n",
        "\n",
        "Gunakan Conv1D untuk mengekstrak fitur lokal dari sekuens:\n",
        "\n",
        "```python\n",
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv1D(32, 5, activation=\"relu\", input_shape=[None, 1]),\n",
        "    keras.layers.MaxPooling1D(2),\n",
        "    keras.layers.LSTM(32),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "```\n",
        "\n",
        "## 6. Bidirectional RNN\n",
        "RNN yang membaca sekuens maju dan mundur, lalu menggabungkan hasilnya. Berguna jika seluruh sekuens tersedia.\n",
        "\n",
        "\n",
        "## 7. Masking dan Padding\n",
        "Untuk sekuens panjang berbeda:\n",
        "- Padding menambahkan 0 agar panjang sekuens sama.\n",
        "- Masking memberitahu model untuk mengabaikan padding.\n",
        "\n"
      ],
      "metadata": {
        "id": "B4a-LuffQa-P"
      },
      "id": "B4a-LuffQa-P"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}