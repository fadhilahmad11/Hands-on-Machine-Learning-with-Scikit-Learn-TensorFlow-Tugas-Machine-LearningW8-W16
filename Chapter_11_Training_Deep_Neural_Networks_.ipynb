{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fadhilahmad11/Hands-on-Machine-Learning-with-Scikit-Learn-TensorFlow-Tugas-Machine-LearningW8-W16/blob/main/Chapter_11_Training_Deep_Neural_Networks_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "989f4af2",
      "metadata": {
        "id": "989f4af2"
      },
      "source": [
        "\n",
        "# Laporan Chapter 11: Training Deep Neural Networks  \n",
        "\n",
        "---\n",
        "\n",
        "## 1. Pendahuluan  \n",
        "\n",
        "Chapter ini membahas teknik dan tantangan dalam melatih Deep Neural Networks (DNN).  \n",
        "Semakin dalam arsitektur neural network, semakin sulit training karena masalah seperti vanishing/exploding gradients.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3672ac75",
      "metadata": {
        "id": "3672ac75"
      },
      "source": [
        "\n",
        "## 2. Vanishing dan Exploding Gradients  \n",
        "\n",
        "Saat backpropagation, gradien bisa sangat kecil (vanish) atau sangat besar (explode).  \n",
        "Akibatnya:  \n",
        "- Vanishing: layer awal hampir tidak belajar.  \n",
        "- Exploding: parameter menjadi tidak stabil.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1113985d",
      "metadata": {
        "id": "1113985d"
      },
      "source": [
        "\n",
        "## 3. Weight Initialization  \n",
        "\n",
        "Inisialisasi bobot penting untuk mencegah vanishing/exploding gradients.  \n",
        "\n",
        "### He Initialization (cocok untuk ReLU)\n",
        "$\n",
        "\\text{Var}(w) = \\frac{2}{n_{\\text{in}}}\n",
        "$\n",
        "\n",
        "di mana:\n",
        "- $( n_{\\text{in}} $): jumlah input ke neuron.\n",
        "\n",
        "Contoh Keras:\n",
        "```python\n",
        "keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c017f7",
      "metadata": {
        "id": "00c017f7"
      },
      "source": [
        "\n",
        "## 4. Batch Normalization  \n",
        "\n",
        "Batch Normalization (BN) menormalkan output layer agar mean ≈ 0 dan variance ≈ 1 pada setiap mini-batch.\n",
        "\n",
        "$\n",
        "\\hat{z}^{(i)} = \\frac{z^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
        "$\n",
        "\n",
        "di mana:\n",
        "- $( \\mu_B $): mean mini-batch\n",
        "- $( \\sigma_B^2 $): variance mini-batch\n",
        "- $( \\epsilon $): nilai kecil untuk stabilitas numerik\n",
        "\n",
        "BN membantu mempercepat training dan mengurangi ketergantungan pada inisialisasi.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f84d221a",
      "metadata": {
        "id": "f84d221a"
      },
      "source": [
        "\n",
        "## 5. Gradient Clipping  \n",
        "\n",
        "Membatasi nilai gradien agar tidak explode:\n",
        "```python\n",
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8148b6a1",
      "metadata": {
        "id": "8148b6a1"
      },
      "source": [
        "\n",
        "## 6. Early Stopping  \n",
        "\n",
        "Berhenti training jika validasi loss berhenti membaik.\n",
        "\n",
        "```python\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "128af11b",
      "metadata": {
        "id": "128af11b"
      },
      "source": [
        "\n",
        "## 7. Learning Rate Schedules  \n",
        "\n",
        "Mengubah learning rate selama training:\n",
        "- **Time-based decay:**\n",
        "$\n",
        "\\eta_t = \\frac{\\eta_0}{1 + \\text{decay} \\cdot t}\n",
        "$\n",
        "\n",
        "- **Exponential decay:**\n",
        "$\n",
        "\\eta_t = \\eta_0 \\cdot e^{-\\text{decay} \\cdot t}\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c97f568",
      "metadata": {
        "id": "3c97f568"
      },
      "source": [
        "\n",
        "## 8. Optimizers  \n",
        "\n",
        "- **Momentum:** mempercepat training, mengurangi osilasi.  \n",
        "- **Nesterov Accelerated Gradient (NAG):** antisipasi arah gradien.  \n",
        "- **Adam:** kombinasi momentum + RMSProp, sangat populer.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}