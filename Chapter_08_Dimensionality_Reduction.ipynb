{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fadhilahmad11/Hands-on-Machine-Learning-with-Scikit-Learn-TensorFlow-Tugas-Machine-LearningW8-W16/blob/main/Chapter_08_Dimensionality_Reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b72615b8",
      "metadata": {
        "id": "b72615b8"
      },
      "source": [
        "# Laporan Chapter 8: Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e113252a",
      "metadata": {
        "id": "e113252a"
      },
      "source": [
        "## 1. Pendahuluan\n",
        "Dimensionality reduction adalah proses menyederhanakan jumlah fitur dalam dataset sambil tetap mempertahankan informasi yang paling penting. Ini penting karena:\n",
        "- Mempercepat proses pelatihan model\n",
        "- Mengurangi risiko overfitting\n",
        "- Mempermudah visualisasi data\n",
        "Namun, proses ini juga dapat menyebabkan hilangnya informasi."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "023133b0",
      "metadata": {
        "id": "023133b0"
      },
      "source": [
        "## 2. Curse of Dimensionality\n",
        "Masalah utama dalam data berdimensi tinggi adalah jarak antar titik menjadi semakin besar, yang membuat model kesulitan untuk membuat prediksi yang akurat. Fenomena ini dikenal sebagai curse of dimensionality. Misalnya, di ruang 1 juta dimensi, jarak rata-rata antara dua titik bisa mencapai lebih dari 400 unit, meskipun mereka berada dalam hypercube berdimensi 1x1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be0ff95b",
      "metadata": {
        "id": "be0ff95b"
      },
      "source": [
        "## 3. Pendekatan Utama\n",
        "Terdapat dua pendekatan utama dalam dimensionality reduction:\n",
        "- **Projection**: Memproyeksikan data ke subruang berdimensi lebih rendah\n",
        "- **Manifold Learning**: Mengasumsikan data berada di permukaan berdimensi rendah dalam ruang berdimensi tinggi (contohnya: Swiss roll)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a1feb35",
      "metadata": {
        "id": "3a1feb35"
      },
      "source": [
        "## 4. Principal Component Analysis (PCA)\n",
        "PCA adalah algoritma dimensionality reduction paling populer. Tujuannya adalah mencari sumbu (komponen utama) yang mempertahankan variansi data paling besar."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7070c61",
      "metadata": {
        "id": "d7070c61"
      },
      "source": [
        "### a. Proyeksi ke Hyperplane\n",
        "```python\n",
        "X_centered = X - X.mean(axis=0)\n",
        "U, s, Vt = np.linalg.svd(X_centered)\n",
        "c1 = Vt.T[:, 0]  # Komponen utama pertama\n",
        "c2 = Vt.T[:, 1]  # Komponen utama kedua\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "836e45a3",
      "metadata": {
        "id": "836e45a3"
      },
      "source": [
        "### b. Proyeksi ke d Dimensi\n",
        "```python\n",
        "W2 = Vt.T[:, :2]\n",
        "X2D = X_centered.dot(W2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f25abf59",
      "metadata": {
        "id": "f25abf59"
      },
      "source": [
        "### c. PCA dengan Scikit-Learn\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "X2D = pca.fit_transform(X)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa0c9479",
      "metadata": {
        "id": "aa0c9479"
      },
      "source": [
        "### d. Menentukan Dimensi Berdasarkan Variansi\n",
        "```python\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(cumsum >= 0.95) + 1\n",
        "```\n",
        "Atau langsung:\n",
        "```python\n",
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c9d4f4",
      "metadata": {
        "id": "27c9d4f4"
      },
      "source": [
        "## 5. Kernel PCA\n",
        "```python\n",
        "from sklearn.decomposition import KernelPCA\n",
        "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
        "X_reduced = rbf_pca.fit_transform(X)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d3818c2",
      "metadata": {
        "id": "8d3818c2"
      },
      "source": [
        "## 6. Incremental PCA dan Randomized PCA\n",
        "Penjelasan tambahan:\n",
        "- **Incremental PCA** cocok untuk dataset besar.\n",
        "- **Randomized PCA** berguna saat membutuhkan estimasi cepat dari komponen utama."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69332b3a",
      "metadata": {
        "id": "69332b3a"
      },
      "source": [
        "## 7. LLE (Locally Linear Embedding)\n",
        "```python\n",
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
        "X_reduced = lle.fit_transform(X)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a8353d5",
      "metadata": {
        "id": "5a8353d5"
      },
      "source": [
        "## 8. Teknik Lain\n",
        "- **t-SNE**: Fokus menjaga kedekatan lokal.\n",
        "- **MDS**: Menjaga jarak antar titik asli.\n",
        "- **Isomap**: Gunakan jarak geodesik.\n",
        "- **Random Projection** dan **LDA** juga termasuk teknik populer lainnya."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6f90f72",
      "metadata": {
        "id": "a6f90f72"
      },
      "source": [
        "## 9. Aplikasi pada MNIST\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_reduced[:60000], y[:60000])\n",
        "print(clf.score(X_reduced[60000:], y[60000:]))\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}